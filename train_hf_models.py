#!/usr/bin/env python3
"""
Script d'entraÃ®nement AMÃ‰LIORÃ‰ pour modÃ¨les XGBoost et LightGBM
OptimisÃ© pour gÃ©rer le dÃ©sÃ©quilibre de classes extrÃªme (10% positifs)

AMÃ‰LIORATIONS PRINCIPALES:
1. Utilise les modÃ¨les amÃ©liorÃ©s avec SMOTE, ADASYN
2. Optimisation automatique des seuils de dÃ©cision
3. Ensemble de modÃ¨les pour LightGBM
4. Cross-validation stratifiÃ©e
5. MÃ©triques mÃ©tier (F2-score favorisant le recall)
6. Analyse dÃ©taillÃ©e des performances

Usage:
    python train_hf_models.py --data data/BTC_USDT_5m.csv
    python train_hf_models.py --limit 20000 --threshold 0.003
"""

import sys
import os
import argparse
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import ConfigLoader
from core.binance_client import BinanceClient
from data.data_loader import DataLoader
from ml_models.xgboost_model import XGBoostModel
from ml_models.lightgbm_model import LightGBMModel
from ml_models.feature_engineering import FeatureEngineer
from utils.logger import setup_logger
from utils.config_helpers import get_nested_config
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score


class HFModelTrainer:
    """
    Trainer optimisÃ© pour dÃ©sÃ©quilibre de classes extrÃªme
    Utilise les techniques avancÃ©es des modÃ¨les amÃ©liorÃ©s
    """

    def __init__(self, config: dict):
        self.config = config
        self.logger = setup_logger('HFModelTrainer')

        # Configuration
        ml_config = config.get('ml', {})
        training_config = ml_config.get('training', {})

        self.min_samples = training_config.get('min_samples', 1000)
        self.validation_split = training_config.get('validation_split', 0.2)
        self.test_split = training_config.get('test_split', 0.1)

        # ParamÃ¨tres de target
        self.horizon_bars = 5
        self.target_threshold = 0.003

        # Initialiser composants
        self.feature_engineer = FeatureEngineer(config)
        self.xgboost = XGBoostModel(config)
        self.lightgbm = LightGBMModel(config)

        self.logger.info("âœ… HF Model Trainer initialisÃ©")
        self.logger.info(f"   Target: {self.target_threshold*100:.1f}% sur {self.horizon_bars} bougies")
        self.logger.info(f"   Techniques: SMOTE, ADASYN, Ensemble, Seuil optimisÃ©")

    def create_target_with_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """CrÃ©e le target et analyse sa distribution"""

        df = df.copy()

        # Future return sur horizon
        df['future_return'] = df['close'].shift(-self.horizon_bars) / df['close'] - 1

        # Target binaire
        df['target'] = (df['future_return'] > self.target_threshold).astype(int)

        # Analyse dÃ©taillÃ©e
        df_clean = df.dropna()
        target_counts = df_clean['target'].value_counts()
        total = len(df_clean)

        if total > 0:
            pct_positive = target_counts.get(1, 0) / total * 100
            pct_negative = target_counts.get(0, 0) / total * 100

            self.logger.info(f"\nğŸ“Š ANALYSE DE LA DISTRIBUTION TARGET:")
            self.logger.info(f"   - Ã‰chantillons totaux: {total}")
            self.logger.info(f"   - UP (>{self.target_threshold*100:.1f}%): {target_counts.get(1, 0)} ({pct_positive:.1f}%)")
            self.logger.info(f"   - DOWN/NEUTRAL: {target_counts.get(0, 0)} ({pct_negative:.1f}%)")

            if pct_positive < 5:
                self.logger.warning(f"âš ï¸ TRÃˆS PEU de positifs! ConsidÃ©rer:")
                self.logger.warning(f"   - RÃ©duire le seuil (actuellement {self.target_threshold*100:.1f}%)")
                self.logger.warning(f"   - Augmenter l'horizon (actuellement {self.horizon_bars} bougies)")
            elif pct_positive < 10:
                self.logger.warning(f"âš ï¸ DÃ©sÃ©quilibre sÃ©vÃ¨re - Techniques avancÃ©es activÃ©es:")
                self.logger.warning(f"   - SMOTE/ADASYN pour rÃ©Ã©quilibrage")
                self.logger.warning(f"   - Optimisation du seuil de dÃ©cision")
            elif pct_positive < 20:
                self.logger.info(f"âœ… DÃ©sÃ©quilibre modÃ©rÃ© - gÃ©rable avec techniques avancÃ©es")
            else:
                self.logger.info(f"âœ… Distribution Ã©quilibrÃ©e")

        return df

    def analyze_model_performance(self, results: dict):
        """Analyse dÃ©taillÃ©e des performances et recommandations"""

        self.logger.info("\n" + "="*70)
        self.logger.info("ğŸ“Š ANALYSE DES PERFORMANCES")
        self.logger.info("="*70)

        # Analyser XGBoost
        if 'xgboost' in results and 'error' not in results['xgboost']:
            xgb = results['xgboost']
            self.logger.info("\nğŸŒ³ XGBoost:")

            val_f1 = xgb.get('val_f1', 0)
            val_precision = xgb.get('val_precision', 0)
            val_recall = xgb.get('val_recall', 0)

            self.logger.info(f"   F1: {val_f1:.3f} | Precision: {val_precision:.3f} | Recall: {val_recall:.3f}")

            if val_f1 < 0.2:
                self.logger.warning("   âš ï¸ F1 trÃ¨s faible - Le modÃ¨le peine Ã  dÃ©tecter les positifs")
            elif val_f1 < 0.4:
                self.logger.info("   ğŸ“ˆ F1 acceptable pour dÃ©sÃ©quilibre sÃ©vÃ¨re")
            else:
                self.logger.info("   âœ… Excellente performance!")

        # Analyser LightGBM
        if 'lightgbm' in results and 'error' not in results['lightgbm']:
            lgb = results['lightgbm']
            self.logger.info("\nğŸ’¡ LightGBM Ensemble:")

            val_f1 = lgb.get('val_f1', 0)
            val_precision = lgb.get('val_precision', 0)
            val_recall = lgb.get('val_recall', 0)

            self.logger.info(f"   F1: {val_f1:.3f} | Precision: {val_precision:.3f} | Recall: {val_recall:.3f}")

            if 'cv_scores' in lgb:
                cv = lgb['cv_scores']
                self.logger.info(f"   Cross-validation F1: {cv['mean_f1']:.3f} +/- {cv['std_f1']:.3f}")

        # Recommandations
        self.logger.info("\nğŸ¯ RECOMMANDATIONS:")

        xgb_f1 = results.get('xgboost', {}).get('val_f1', 0)
        lgb_f1 = results.get('lightgbm', {}).get('val_f1', 0)

        if xgb_f1 > lgb_f1 * 1.2:
            self.logger.info("   â†’ XGBoost meilleur - Utiliser comme modÃ¨le principal")
        elif lgb_f1 > xgb_f1 * 1.2:
            self.logger.info("   â†’ LightGBM Ensemble meilleur - Utiliser comme modÃ¨le principal")
        else:
            self.logger.info("   â†’ Combiner les deux modÃ¨les (moyenne pondÃ©rÃ©e)")

        avg_f1 = (xgb_f1 + lgb_f1) / 2
        if avg_f1 >= 0.4:
            self.logger.info("\nâœ… EXCELLENTES PERFORMANCES! PrÃªt pour le trading.")
        elif avg_f1 >= 0.25:
            self.logger.info("\nğŸ“ˆ PERFORMANCES CORRECTES. Utilisable avec prudence.")
        else:
            self.logger.warning("\nâš ï¸ PERFORMANCES FAIBLES - AmÃ©liorations nÃ©cessaires:")
            self.logger.warning("   - Collecter plus de donnÃ©es")
            self.logger.warning("   - Ajuster le seuil de target")
            self.logger.warning("   - Ajouter des features techniques")

    def train_all(self, df: pd.DataFrame) -> dict:
        """EntraÃ®ne tous les modÃ¨les avec techniques avancÃ©es"""

        self.logger.info("="*70)
        self.logger.info("ğŸš€ DÃ‰BUT ENTRAÃNEMENT MODÃˆLES AMÃ‰LIORÃ‰S")
        self.logger.info("="*70)

        start_time = datetime.now()

        # 1. Feature engineering
        self.logger.info("\nğŸ”¨ GÃ©nÃ©ration des features...")
        df_features = self.feature_engineer.generate_features(df)

        if df_features.empty:
            self.logger.error("âŒ Ã‰chec gÃ©nÃ©ration features")
            return {}

        # 2. CrÃ©er target avec analyse
        self.logger.info(f"\nğŸ¯ CrÃ©ation target (threshold={self.target_threshold*100:.1f}%)...")
        df_features = self.create_target_with_analysis(df_features)

        # 3. Nettoyer NaN
        df_features = df_features.dropna()

        if len(df_features) < self.min_samples:
            self.logger.error(f"âŒ Pas assez de donnÃ©es: {len(df_features)} < {self.min_samples}")
            return {}

        self.logger.info(f"\nâœ… {len(df_features)} samples prÃªts pour entraÃ®nement")

        # 4. Identifier features
        feature_names = self.feature_engineer.get_feature_names(df_features)
        X = df_features[feature_names]
        y = df_features['target']

        # 5. Split train/test
        test_size = int(len(X) * self.test_split)
        X_train = X.iloc[:-test_size]
        y_train = y.iloc[:-test_size]
        X_test = X.iloc[-test_size:]
        y_test = y.iloc[-test_size:]

        self.logger.info(f"ğŸ“Š Split: Train={len(X_train)} | Test={len(X_test)}")

        # Afficher distribution
        n_pos = y_train.sum()
        n_neg = len(y_train) - n_pos
        pos_ratio = n_pos / len(y_train)
        self.logger.info(f"âš–ï¸ Classes: {n_neg} nÃ©gatifs / {n_pos} positifs ({pos_ratio*100:.1f}% positifs)")

        results = {}

        # 6. EntraÃ®ner XGBoost amÃ©liorÃ©
        self.logger.info("\n" + "="*50)
        self.logger.info("ğŸŒ³ ENTRAÃNEMENT XGBOOST AMÃ‰LIORÃ‰")
        self.logger.info("="*50)

        try:
            # Configurer stratÃ©gie selon dÃ©sÃ©quilibre
            if pos_ratio < 0.1:
                self.xgboost.resampling_strategy = 'smote'
            else:
                self.xgboost.resampling_strategy = 'none'

            xgb_metrics = self.xgboost.train(
                X_train, y_train,
                validation_split=self.validation_split,
                verbose=False
            )

            # Test
            y_test_pred = self.xgboost.predict(X_test)

            xgb_metrics['test_accuracy'] = accuracy_score(y_test, y_test_pred)
            xgb_metrics['test_f1'] = f1_score(y_test, y_test_pred, zero_division=0)
            xgb_metrics['test_precision'] = precision_score(y_test, y_test_pred, zero_division=0)
            xgb_metrics['test_recall'] = recall_score(y_test, y_test_pred, zero_division=0)

            # Sauvegarder
            xgb_path = self.xgboost.save()
            xgb_metrics['model_path'] = xgb_path

            results['xgboost'] = xgb_metrics

            self.logger.info(f"\nâœ… XGBoost entraÃ®nÃ© (seuil: {self.xgboost.optimal_threshold:.3f}):")
            self.logger.info(f"   Test: F1={xgb_metrics['test_f1']:.3f}, P={xgb_metrics['test_precision']:.3f}, R={xgb_metrics['test_recall']:.3f}")

        except Exception as e:
            self.logger.error(f"âŒ Erreur XGBoost: {e}")
            import traceback
            traceback.print_exc()
            results['xgboost'] = {'error': str(e)}

        # 7. EntraÃ®ner LightGBM ensemble amÃ©liorÃ©
        self.logger.info("\n" + "="*50)
        self.logger.info("ğŸ’¡ ENTRAÃNEMENT LIGHTGBM ENSEMBLE AMÃ‰LIORÃ‰")
        self.logger.info("="*50)

        try:
            # Configurer selon dÃ©sÃ©quilibre
            if pos_ratio < 0.1:
                self.lightgbm.resampling_strategy = 'adasyn'
            else:
                self.lightgbm.resampling_strategy = 'borderline'

            lgb_metrics = self.lightgbm.train(
                X_train, y_train,
                validation_split=self.validation_split,
                verbose=False
            )

            # Test
            y_test_pred = self.lightgbm.predict(X_test)

            lgb_metrics['test_accuracy'] = accuracy_score(y_test, y_test_pred)
            lgb_metrics['test_f1'] = f1_score(y_test, y_test_pred, zero_division=0)
            lgb_metrics['test_precision'] = precision_score(y_test, y_test_pred, zero_division=0)
            lgb_metrics['test_recall'] = recall_score(y_test, y_test_pred, zero_division=0)

            # Sauvegarder
            lgb_path = self.lightgbm.save()
            lgb_metrics['model_path'] = lgb_path

            results['lightgbm'] = lgb_metrics

            self.logger.info(f"\nâœ… LightGBM entraÃ®nÃ© (ensemble {self.lightgbm.n_models} modÃ¨les, seuil: {self.lightgbm.ensemble_threshold:.3f}):")
            self.logger.info(f"   Test: F1={lgb_metrics['test_f1']:.3f}, P={lgb_metrics['test_precision']:.3f}, R={lgb_metrics['test_recall']:.3f}")

        except Exception as e:
            self.logger.error(f"âŒ Erreur LightGBM: {e}")
            import traceback
            traceback.print_exc()
            results['lightgbm'] = {'error': str(e)}

        # 8. MÃ©ta-donnÃ©es
        elapsed = (datetime.now() - start_time).total_seconds()
        results['training_time_seconds'] = elapsed
        results['samples_trained'] = len(X_train)
        results['samples_tested'] = len(X_test)
        results['target_threshold'] = self.target_threshold
        results['horizon_bars'] = self.horizon_bars
        results['positive_ratio'] = float(pos_ratio)

        # 9. Analyse des performances
        self.analyze_model_performance(results)

        # 10. RÃ©sumÃ© final
        self._print_summary(results)

        return results

    def _print_summary(self, results: dict):
        """Affiche le rÃ©sumÃ© final"""

        self.logger.info("\n" + "="*70)
        self.logger.info("âœ… ENTRAÃNEMENT TERMINÃ‰")
        self.logger.info("="*70)

        self.logger.info(f"\nğŸ“Š Configuration:")
        self.logger.info(f"   - Target threshold: {results.get('target_threshold', 0)*100:.1f}%")
        self.logger.info(f"   - Horizon: {results.get('horizon_bars', 0)} bougies")
        self.logger.info(f"   - Samples entraÃ®nÃ©s: {results.get('samples_trained', 0)}")
        self.logger.info(f"   - DurÃ©e: {results.get('training_time_seconds', 0):.1f}s")

        self.logger.info(f"\nğŸ“ˆ PERFORMANCES FINALES:")

        xgb_f1 = results.get('xgboost', {}).get('test_f1', 0)
        lgb_f1 = results.get('lightgbm', {}).get('test_f1', 0)

        if 'xgboost' in results and 'error' not in results['xgboost']:
            xgb = results['xgboost']
            self.logger.info(f"\n   ğŸŒ³ XGBoost:")
            self.logger.info(f"      Test F1:       {xgb.get('test_f1', 0):.2%}")
            self.logger.info(f"      Test Precision: {xgb.get('test_precision', 0):.2%}")
            self.logger.info(f"      Test Recall:    {xgb.get('test_recall', 0):.2%}")

        if 'lightgbm' in results and 'error' not in results['lightgbm']:
            lgb = results['lightgbm']
            self.logger.info(f"\n   ğŸ’¡ LightGBM Ensemble:")
            self.logger.info(f"      Test F1:       {lgb.get('test_f1', 0):.2%}")
            self.logger.info(f"      Test Precision: {lgb.get('test_precision', 0):.2%}")
            self.logger.info(f"      Test Recall:    {lgb.get('test_recall', 0):.2%}")

        if xgb_f1 > 0 and lgb_f1 > 0:
            combined = (xgb_f1 * 0.6 + lgb_f1 * 0.4)
            self.logger.info(f"\n   ğŸ¯ Combined (60/40): {combined:.2%}")


def load_data_from_csv(csv_path: str, logger) -> pd.DataFrame:
    """Charge les donnÃ©es depuis un fichier CSV"""

    path = Path(csv_path)

    if not path.exists():
        logger.error(f"âŒ Fichier non trouvÃ©: {csv_path}")
        return pd.DataFrame()

    logger.info(f"ğŸ“‚ Chargement: {csv_path}")

    try:
        df = pd.read_csv(csv_path, index_col=0, parse_dates=True)

        required = ['open', 'high', 'low', 'close', 'volume']
        missing = [c for c in required if c not in df.columns]

        if missing:
            logger.error(f"âŒ Colonnes manquantes: {missing}")
            return pd.DataFrame()

        logger.info(f"âœ… {len(df)} bougies chargÃ©es")
        logger.info(f"   PÃ©riode: {df.index.min()} â†’ {df.index.max()}")

        return df

    except Exception as e:
        logger.error(f"âŒ Erreur lecture: {e}")
        return pd.DataFrame()


def load_data_from_api(config: dict, limit: int, logger) -> pd.DataFrame:
    """Charge les donnÃ©es depuis l'API Binance"""

    logger.info(f"ğŸŒ Connexion API Binance...")

    client = BinanceClient(config)
    data_loader = DataLoader(client, config)

    symbol = get_nested_config(config, 'symbols', 'primary', default='BTC/USDT')

    logger.info(f"ğŸ“Š Chargement {symbol} 5m ({limit} bougies)...")

    df = data_loader.load_historical_data(
        symbol=symbol,
        timeframe='5m',
        limit=limit
    )

    if not df.empty:
        logger.info(f"âœ… {len(df)} bougies chargÃ©es")
        logger.info(f"   PÃ©riode: {df.index.min()} â†’ {df.index.max()}")

    return df


def main():
    """Point d'entrÃ©e principal"""

    parser = argparse.ArgumentParser(
        description='EntraÃ®nement de modÃ¨les ML amÃ©liorÃ©s pour trading'
    )
    parser.add_argument('--data', type=str, default=None,
                       help='Chemin vers fichier CSV')
    parser.add_argument('--limit', type=int, default=10000,
                       help='Nombre de bougies depuis API')
    parser.add_argument('--threshold', type=float, default=0.003,
                       help='Seuil de target en %% (dÃ©faut: 0.3%%)')
    parser.add_argument('--horizon', type=int, default=5,
                       help='Horizon de prÃ©diction en bougies')

    args = parser.parse_args()

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                           â•‘
â•‘            ğŸš€ ENTRAÃNEMENT MODÃˆLES ML AMÃ‰LIORÃ‰S ğŸš€                       â•‘
â•‘                                                                           â•‘
â•‘   Techniques avancÃ©es pour dÃ©sÃ©quilibre de classes:                      â•‘
â•‘   - SMOTE / ADASYN pour rÃ©Ã©chantillonnage                               â•‘
â•‘   - Optimisation automatique des seuils                                  â•‘
â•‘   - Ensemble de modÃ¨les LightGBM                                         â•‘
â•‘   - Cross-validation stratifiÃ©e                                          â•‘
â•‘                                                                           â•‘
â•‘   Target: {:.1f}% sur {} bougies                                              â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """.format(args.threshold * 100, args.horizon))

    logger = setup_logger('HFTraining')

    try:
        # Charger config
        logger.info("ğŸ“‹ Chargement configuration...")
        config_loader = ConfigLoader()
        config = config_loader.config

        # Charger donnÃ©es
        if args.data:
            df = load_data_from_csv(args.data, logger)
        else:
            df = load_data_from_api(config, args.limit, logger)

        if df.empty:
            logger.error("âŒ Pas de donnÃ©es!")
            sys.exit(1)

        # EntraÃ®ner
        trainer = HFModelTrainer(config)
        trainer.target_threshold = args.threshold
        trainer.horizon_bars = args.horizon

        results = trainer.train_all(df)

        if not results:
            logger.error("âŒ Ã‰chec entraÃ®nement")
            sys.exit(1)

        # Instructions finales
        print("\n" + "="*70)
        print("ğŸ’¡ PROCHAINES Ã‰TAPES:")
        print("="*70)

        xgb_f1 = results.get('xgboost', {}).get('test_f1', 0)
        lgb_f1 = results.get('lightgbm', {}).get('test_f1', 0)

        print(f"""
MODÃˆLES SAUVEGARDÃ‰S:
â€¢ XGBoost: {results.get('xgboost', {}).get('model_path', 'N/A')}
â€¢ LightGBM: {results.get('lightgbm', {}).get('model_path', 'N/A')}

PERFORMANCES:
â€¢ XGBoost F1: {xgb_f1:.3f}
â€¢ LightGBM F1: {lgb_f1:.3f}

UTILISATION:
1. Les modÃ¨les sont automatiquement chargÃ©s par MLSignalFilter
2. Lancer le backtest: python paper_trading.py
3. Pour re-entraÃ®ner avec plus de donnÃ©es: python train_hf_models.py --limit 50000
        """)

    except KeyboardInterrupt:
        print("\nâš ï¸ Interruption")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Erreur: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
